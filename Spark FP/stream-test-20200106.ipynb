{"cells": [{"metadata": {"run_control": {"marked": true}, "trusted": false}, "cell_type": "code", "source": "from pyspark.streaming import StreamingContext\nbatchIntervalSeconds = 10 \n\nssc = StreamingContext(spark.sparkContext, batchIntervalSeconds)\n# Set each DStreams in this context to remember RDDs it generated in the last given duration.\n# DStreams remember RDDs only for a limited duration of time and releases them for garbage\n# collection. This method allows the developer to specify how long to remember the RDDs (\n# if the developer wishes to query old data outside the DStream computation).\nssc.remember(60)\nlines = ssc.textFileStream(\"s3://sita-coe-ds-flightaware/\")\n\nssc.sparkSession(lines.pprint()\nwords = lines.flatMap(lambda line: line.split(\",\"))\npairs = words.map(lambda word: (word, 1))\nwordCounts = pairs.reduceByKey(lambda x, y: x + y)\n\ndef process(time, rdd):\n    df = sqlContext.createDataFrame(rdd)\n    df.registerTempTable(\"myCounts\")\n    df.show()\n\n\nwordCounts.foreachRDD(process)\nwordCounts.pprint()\n\ncheckpointDir = \"/mnt/tmp/checkpoint\"\n\n# This line starts the streaming context in the background.\nssc.start()\n\n# This ensures the cell is put on hold until the background streaming thread is started properly.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 2)\n#ssc.stop()\n", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2020-01-06T21:00:51.577Z"}, "run_control": {"marked": true}, "trusted": false}, "cell_type": "code", "source": "#from __future__ import print_function\n\n#import sys\n\n#from pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\nbatchIntervalSeconds = 10 \n\ndef creatingFunc():\n  ssc = StreamingContext(spark.sparkContext, batchIntervalSeconds)\n  # Set each DStreams in this context to remember RDDs it generated in the last given duration.\n  # DStreams remember RDDs only for a limited duration of time and releases them for garbage\n  # collection. This method allows the developer to specify how long to remember the RDDs (\n  # if the developer wishes to query old data outside the DStream computation).\n  ssc.remember(60)\n  lines = ssc.textFileStream(\"s3://sita-coe-ds-pre-prod-v1/stream/flightaware/\")\n  lines.pprint()\n  words = lines.flatMap(lambda line: line.split(\",\"))\n  pairs = words.map(lambda word: (word, 1))\n  wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n  def process(time, rdd):\n    df = sqlContext.createDataFrame(rdd)\n    df.registerTempTable(\"myCounts\")\n  wordCounts.foreachRDD(process)\n  return ssc\n\n\ncheckpointDir = \"/mnt/tmp/checkpoint\"\nssc = StreamingContext.getActiveOrCreate(checkpointDir, creatingFunc)\n\n# This line starts the streaming context in the background.\nssc.start()\n\n# This ensures the cell is put on hold until the background streaming thread is started properly.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 2)\n\nsc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\nssc = StreamingContext(spark.sparkContext, 1)\n\nlines = ssc.textFileStream('s3://sita-coe-ds-pre-prod-v1/stream/flightaware/')\n#counts = lines.flatMap(lambda line: line.split(\" \"))\\\n#              .map(lambda word: (word, 1))\\\n#              .reduceByKey(lambda a, b: a+b)\n\ndef sendRecord(rdd):\n    connection = createNewConnection()  # executed at the driver\n    rdd.foreach(lambda record: connection.send(record))\n    connection.close()\n\n#lines.foreachRDD(print)\n\n\nssc.start()\nssc.awaitTermination()\n", "execution_count": null, "outputs": []}, {"metadata": {"ExecuteTime": {"start_time": "2020-01-06T21:00:51.578Z"}, "trusted": false}, "cell_type": "code", "source": "counts\n", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pyspark3kernel", "display_name": "PySpark3", "language": ""}, "varInspector": {"window_display": false, "cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"library": "var_list.py", "delete_cmd_prefix": "del ", "delete_cmd_postfix": "", "varRefreshCmd": "print(var_dic_list())"}, "r": {"library": "var_list.r", "delete_cmd_prefix": "rm(", "delete_cmd_postfix": ") ", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"]}, "language_info": {"name": "pyspark3", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}}, "nbformat": 4, "nbformat_minor": 2}